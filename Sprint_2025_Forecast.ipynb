{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "296efaff",
   "metadata": {},
   "source": [
    "Digital Epidemiology Dengue Transmission Model\n",
    "==============================================\n",
    "\n",
    "This code implements a comprehensive dengue transmission model for Brazilian States. The model combines:\n",
    "\n",
    "1. A compartmental ODE system modeling dengue transmission between vectors (Aedes aegypti) \n",
    "   and humans\n",
    "2. Weather-dependent parameters affecting vector biology\n",
    "3. Bayesian parameter estimation using PyMC\n",
    "4. Forecasting capabilities with uncertainty quantification\n",
    "5. Integration with epidemiological surveillance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c85e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "from numba import njit\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "from pytensor.compile.ops import as_op\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sympy import exp\n",
    "import arviz as az\n",
    "import scipy.stats as stats\n",
    "import corner\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import mosqlient as mosq\n",
    "from datetime import datetime\n",
    "from geopy import Nominatim\n",
    "from meteostat import Stations, Hourly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d27ac93",
   "metadata": {},
   "source": [
    "API Configuration and Model Parameters\n",
    "=====================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e404d50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dengue surveillance data API configuration\n",
    "infodengue_api = \"https://api.mosqlimate.org/api/datastore/infodengue/\"\n",
    "mosqlimate_key = {\"X-UID-Key\": \"DavideNicola96:772df8a3-414a-438a-93b0-f326fea382e9\"}\n",
    "disease = \"dengue\"\n",
    "\n",
    "# Primary model parameters to be estimated via Bayesian inference\n",
    "par = {\n",
    "    'k_v': 0.466,    # Probability of infection per bite from infected human\n",
    "    'k_h': 0.314,    # Probability of infection per bite from infected vector\n",
    "    'sus_per': 0.128 # Proportion of susceptible population\n",
    "}\n",
    "\n",
    "# Weather-dependent parameters for vector biology modeling\n",
    "dict_weather_coeffs = {\n",
    "    'A': 0.15,\n",
    "    'HA': 33256.,\n",
    "    'HH': 50543.,\n",
    "    'TH': 301.67,\n",
    "    'b0': 5.,\n",
    "    'Emax': 80.,\n",
    "    'Emean': 7.,\n",
    "    'Evar': 2.,\n",
    "    'bite': 0.00161,\n",
    "}\n",
    "\n",
    "# Fixed biological parameters based on literature\n",
    "egg_lper = 0.01      # Percentage of female mosquitoes laying eggs per day\n",
    "female_per = 0.5     # Proportion of female mosquitoes\n",
    "mu_v = 0.02941       # Vector mortality rate (1/days)\n",
    "psi_v = 0.05         # Vertical transmission rate\n",
    "mu_e = 0.15          # Egg mortality rate (1/days)\n",
    "alpha_v = 0.1428     # Vector incubation rate (1/days)\n",
    "pi_h = 1. / 25500.   # Human birth rate (1/days)\n",
    "mu_h = 1. / 25500.   # Human mortality rate (1/days)\n",
    "alpha_h = 0.33       # Human incubation rate (1/days)\n",
    "beta_h = 0.30        # Human recovery rate (1/days)\n",
    "sigma_h = 0.0001     # Disease-induced mortality rate (1/days)\n",
    "\n",
    "# Vectors carrying capacity\n",
    "cc_v = 3\n",
    "\n",
    "# Minimum value to prevent numerical issues\n",
    "MIN_VALUE = 1e-6\n",
    "\n",
    "# Load Brazilian geographic data for geocoding like geocode and numicipality name\n",
    "ods_path = r\"~/codici/RELATORIO_DTB_BRASIL_DISTRITOS.ods\"\n",
    "geo_data = pd.read_excel(ods_path, engine='odf')\n",
    "geo_data.columns = geo_data.iloc[5].values\n",
    "geo_data = geo_data.drop(index=range(0, 6))\n",
    "geo_data = geo_data.reset_index(drop=True)\n",
    "geo_data = geo_data[[\"Nome_UF\",\"Código Município Completo\", \"Nome_Município\"]]\n",
    "geo_data.drop_duplicates(inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92233c47",
   "metadata": {},
   "source": [
    "Data Acquisition Functions\n",
    "==========================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b121f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_dengue_data_state_from_csv(state_geocodes, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch dengue case data for a Brazilian state from local CSV files.\n",
    "    \n",
    "    This function aggregates dengue cases and population data at the state level\n",
    "    for model training and validation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    state_geocodes : list\n",
    "        List of municipality geocodes for the target state\n",
    "    start_date : str\n",
    "        Start date for data extraction (YYYY-MM-DD format)\n",
    "    end_date : str\n",
    "        End date for data extraction (YYYY-MM-DD format)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    weekly_cases : pd.DataFrame\n",
    "        Aggregated weekly dengue cases with population data\n",
    "    major_cities : pd.DataFrame\n",
    "        Top 10 cities by population for weather data sampling\n",
    "    \"\"\"\n",
    "    # Load compressed dengue surveillance data\n",
    "    cases_data = pd.read_csv(\n",
    "        '/home/dnicola/codici/data_sprint_2025/dengue.csv.gz',\n",
    "        compression='gzip',\n",
    "        parse_dates=['date'],\n",
    "        usecols=['date', 'geocode', 'casos', 'epiweek']\n",
    "    )\n",
    "    \n",
    "    # Filter data for target state and date range\n",
    "    cases_data = cases_data[cases_data['geocode'].isin(state_geocodes)]\n",
    "    date_mask = (cases_data['date'] >= start_date) & (cases_data['date'] <= end_date)\n",
    "    cases_data = cases_data[date_mask].reset_index(drop=True)\n",
    "\n",
    "    # Aggregate cases by week (epidemiological surveillance standard)\n",
    "    cases_data['date'] = pd.to_datetime(cases_data['date'])\n",
    "    cases_data.set_index('date', inplace=True)\n",
    "    weekly_cases = cases_data.groupby('date').agg({\n",
    "        'casos': 'sum'\n",
    "    }).reset_index()\n",
    "    weekly_cases.columns = ['data_iniSE', 'casos']\n",
    "\n",
    "    # Load population data for normalization\n",
    "    pop_data = pd.read_csv(\n",
    "        '/home/dnicola/codici/data_sprint_2025/datasus_population_2001_2024.csv.gz',\n",
    "        compression='gzip'\n",
    "    )\n",
    "\n",
    "    # Filter and aggregate population data by year\n",
    "    pop_data = pop_data[pop_data['geocode'].isin(state_geocodes)]\n",
    "    date_mask_year = (pop_data['year'] >= pd.to_datetime(start_date).year) & (pop_data['year'] <= pd.to_datetime(end_date).year)\n",
    "    pop_data = pop_data[date_mask_year].reset_index(drop=True)\n",
    "    pop_data_by_year = pop_data.groupby('year').agg({\n",
    "        'population': 'sum',\n",
    "    }).reset_index()\n",
    "\n",
    "    # Merge cases with population data\n",
    "    weekly_cases['year'] = pd.to_datetime(weekly_cases['data_iniSE']).dt.year\n",
    "    weekly_cases = weekly_cases.merge(pop_data_by_year, on='year', how='left')\n",
    "    weekly_cases = weekly_cases[['data_iniSE', 'casos', 'population']]\n",
    "    weekly_cases.columns = ['data_iniSE', 'casos', 'pop']\n",
    "    weekly_cases = weekly_cases.sort_values('data_iniSE', ascending=True)\n",
    "    weekly_cases = weekly_cases.reset_index(drop=True)\n",
    "    \n",
    "    # Identify major cities for weather data sampling\n",
    "    latest_year = pop_data['year'].max()\n",
    "    latest_pop = pop_data[pop_data['year'] == latest_year].copy()\n",
    "\n",
    "    geo_renamed = geo_data[['Código Município Completo', 'Nome_Município']].rename(\n",
    "        columns={'Código Município Completo': 'geocode'}\n",
    "    )\n",
    "    geo_renamed['geocode'] = geo_renamed['geocode'].astype(int)\n",
    "    \n",
    "    major_cities = latest_pop.merge(\n",
    "        geo_renamed,\n",
    "        on='geocode',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    major_cities = major_cities[['Nome_Município', 'geocode', 'population']]\n",
    "    major_cities.columns = ['city', 'geocode', 'pop']\n",
    "    major_cities = major_cities.nlargest(10, 'pop').reset_index(drop=True)\n",
    "    \n",
    "    return weekly_cases, major_cities\n",
    "\n",
    "def get_state_weather_data(start_date, end_date, weather_coeffs, major_cities):\n",
    "    \"\"\"\n",
    "    Collect and average weather data from the 10 most populated cities in a state.\n",
    "    \n",
    "    Weather parameters are crucial for dengue transmission modeling as they\n",
    "    affect vector biology (development rates, survival, biting behavior).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_date : str\n",
    "        Start date for weather data\n",
    "    end_date : str\n",
    "        End date for weather data\n",
    "    weather_coeffs : dict\n",
    "        Coefficients for weather-dependent biological processes\n",
    "    major_cities : pd.DataFrame\n",
    "        Cities to sample weather data from\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    avg_weather_data : pd.DataFrame\n",
    "        State-averaged weather-dependent biological parameters\n",
    "    \"\"\"\n",
    "    weather_data_list = []\n",
    "    successful_cities = []\n",
    "    \n",
    "    # Collect weather data from multiple cities to reduce spatial bias\n",
    "    for city in major_cities['city']:\n",
    "        weather_data = weather_functions_Aedes(\n",
    "            city, 'Brasil', \n",
    "            pd.to_datetime(start_date), \n",
    "            pd.to_datetime(end_date), \n",
    "            weather_coeffs\n",
    "        )\n",
    "        weather_data_list.append(weather_data)\n",
    "        successful_cities.append(city)\n",
    "    \n",
    "    # Average weather-dependent parameters across cities\n",
    "    avg_weather_data = weather_data_list[0].copy()\n",
    "    for col in ['bite', 'egg', 'theta']:\n",
    "        if col in avg_weather_data.columns:\n",
    "            col_data = np.array([df[col].values for df in weather_data_list])\n",
    "            avg_weather_data[col] = np.mean(col_data, axis=0)\n",
    "    \n",
    "    return avg_weather_data\n",
    "\n",
    "def Briere(T, a, b, c):\n",
    "    \"\"\"\n",
    "    Brière temperature response function for arthropod biology.\n",
    "    \n",
    "    This nonlinear function captures the temperature dependence of biological\n",
    "    rates in ectothermic organisms like Aedes aegypti mosquitoes.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    T : float/array\n",
    "        Temperature (°C)\n",
    "    a : float\n",
    "        Rate coefficient\n",
    "    b : float\n",
    "        Lower temperature threshold\n",
    "    c : float\n",
    "        Upper temperature threshold\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    rate : float/array\n",
    "        Temperature-dependent biological rate\n",
    "    \"\"\"\n",
    "    T = np.clip(T, b, c)\n",
    "    return a * T * (T - b) * np.sqrt(c - T)\n",
    "\n",
    "def get_location(city_name, country_name):\n",
    "    \"\"\"\n",
    "    Geocode city location for weather station selection.\n",
    "    \n",
    "    Uses Nominatim geocoding service with rate limiting to avoid\n",
    "    service overload.\n",
    "    \"\"\"\n",
    "    time.sleep(3)  # Rate limiting\n",
    "\n",
    "    geolocator = Nominatim(user_agent=\"weather_app\", timeout=3)\n",
    "    location = geolocator.geocode(f\"{city_name}, {country_name}\")\n",
    "    \n",
    "    return location\n",
    "\n",
    "def get_altitude(latitude, longitude):\n",
    "    \"\"\"\n",
    "    Get elevation data for evaporation calculations.\n",
    "    \n",
    "    Altitude affects temperature and humidity patterns, which influence\n",
    "    mosquito breeding site availability.\n",
    "    \"\"\"\n",
    "    elevation_url = \"https://api.opentopodata.org/v1/test-dataset\"\n",
    "    elevation_params = {\n",
    "        \"locations\": f\"{latitude},{longitude}\"\n",
    "    }\n",
    "    elevation_response = requests.get(elevation_url, params=elevation_params).json()\n",
    "    altitude = elevation_response['results'][0]['elevation']\n",
    "\n",
    "    return altitude\n",
    "\n",
    "def get_nearest_station(city_name, country_name, end_date):\n",
    "    \"\"\"\n",
    "    Find nearest meteorological station with data coverage.\n",
    "    \n",
    "    Prioritizes stations with complete temporal coverage for the\n",
    "    analysis period.\n",
    "    \"\"\"\n",
    "    end_date = end_date.strftime(\"%Y-%m-%d\")\n",
    "    location = get_location(city_name, country_name)\n",
    "\n",
    "    if location:\n",
    "        stations = Stations()\n",
    "        nearest_station = stations.nearby(location.latitude, location.longitude).fetch()\n",
    "        nearest_station.dropna(subset=[\"hourly_end\"], inplace=True)\n",
    "        nearest_station = nearest_station[(nearest_station[\"hourly_end\"] >= end_date) & (nearest_station[\"hourly_start\"] <= end_date)]\n",
    "        return nearest_station\n",
    "    else:\n",
    "        raise ValueError(f\"Coordinates not found for {city_name}, {country_name}\")\n",
    "\n",
    "def fetch_weather_data(city_name, country_name, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Download meteorological data from nearest weather station.\n",
    "    \n",
    "    Retrieves daily temperature, humidity, precipitation, and wind data\n",
    "    needed for vector biology modeling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        i = 0\n",
    "        while True:\n",
    "            nearest_station = get_nearest_station(city_name, country_name, end_date).iloc[[i]]\n",
    "            # Resample hourly data to daily means and fill missing values\n",
    "            data = Hourly(nearest_station, start=start_date, end=end_date).normalize().fetch().resample('D').mean().fillna(0)\n",
    "            data = data[['temp', 'dwpt', 'rhum', 'prcp', 'wdir', 'wspd']]\n",
    "            if not data.empty:\n",
    "                return data\n",
    "            else:\n",
    "                i += 1\n",
    "    except ValueError as e:\n",
    "        print(f\"Error finding nearest station for {city_name}, {country_name}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching weather data: {e}\")\n",
    "    return None\n",
    "\n",
    "def weather_functions_Aedes(city_name, country_name, start_date, end_date, coeffs):\n",
    "    \"\"\"\n",
    "    Calculate weather-dependent Aedes aegypti biological parameters.\n",
    "    \n",
    "    This function transforms meteorological data into biologically meaningful\n",
    "    parameters for the dengue transmission model:\n",
    "    - theta: temperature-dependent development rate\n",
    "    - bite: temperature-dependent biting rate  \n",
    "    - egg: moisture-dependent egg laying rate\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    city_name : str\n",
    "        Target city name\n",
    "    country_name : str\n",
    "        Target country\n",
    "    start_date : datetime\n",
    "        Start date for weather data\n",
    "    end_date : datetime\n",
    "        End date for weather data\n",
    "    coeffs : dict\n",
    "        Weather response coefficients\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    weather_data : pd.DataFrame\n",
    "        Daily weather data with calculated biological parameters\n",
    "    \"\"\"\n",
    "    K = 273.15  # Kelvin conversion constant\n",
    "\n",
    "    A, HA, HH, TH, b0, Emax, Emean, Evar, bite = coeffs.values()\n",
    "    location = get_location(city_name, country_name)\n",
    "    latitude, longitude = location.latitude, location.longitude\n",
    "    altitude = get_altitude(latitude, longitude)\n",
    "\n",
    "    weather_data = fetch_weather_data(city_name, country_name, start_date, end_date)\n",
    "    # Smooth temperature with 7-day rolling mean to reduce noise\n",
    "    weather_data['temp_r'] = weather_data['temp'].rolling(7, min_periods=1).mean()\n",
    "\n",
    "    # Initialize biological parameter arrays\n",
    "    Theta = np.zeros(shape = weather_data.shape[0], dtype = np.float64)\n",
    "    Egg = np.zeros(shape = weather_data.shape[0], dtype = np.float64)\n",
    "    Bite = np.zeros(shape = weather_data.shape[0], dtype = np.float64)\n",
    "    \n",
    "    for t in range(weather_data.shape[0]):\n",
    "        Temp = weather_data['temp_r'][t]\n",
    "    \n",
    "        # Calculation of development rate\n",
    "        Theta[t] = A * ((Temp + K) / 298.15) * exp((HA / (1.987)) * (1 / 298.15 - 1 / (Temp + K))) * \\\n",
    "            (1 + exp((HH / (1.987)) * (1 / TH - 1 / (Temp + K))))\n",
    "            \n",
    "        # Brière function for biting rate\n",
    "        Bite[t] = Briere(Temp, bite, 13.35, 40.08)\n",
    "\n",
    "        # Moisture-dependent egg laying (requires 3-day accumulation)\n",
    "        if t >= 3:\n",
    "            Precipitation = 0\n",
    "            Evaporation = 0\n",
    "            # Calculate 3-day moisture balance\n",
    "            for d in range(t - 2, t + 1):\n",
    "                temp = weather_data['temp'][d]\n",
    "                Td = weather_data['dwpt'][d]\n",
    "\n",
    "                Precipitation += weather_data.iloc[d,3]\n",
    "                Evaporation += (700 * (temp + 0.006 * altitude)) / ((100 - latitude) * (80 - temp)) + 15 * (temp - Td) / (80 - temp)\n",
    "\n",
    "            Moisture = Precipitation - Evaporation\n",
    "            # Sigmoid response to moisture availability\n",
    "            Egg[t] = b0 + Emax / (1 + exp(-(Moisture - Emean) / Evar))\n",
    "        \n",
    "    weather_data['theta'] = Theta\n",
    "    weather_data['egg'] = Egg\n",
    "    weather_data['bite'] = Bite\n",
    "    \n",
    "    # Remove first 3 days (incomplete moisture calculations)\n",
    "    weather_data = weather_data.iloc[3:,:]\n",
    "    \n",
    "    return weather_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d5c7f3",
   "metadata": {},
   "source": [
    "Dengue Transmission Model (Compartmental ODE System)\n",
    "===================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c522ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(fastmath=True, cache=True)\n",
    "def ode_system(y, k_v, k_h, pi_v, theta_v, b):\n",
    "    \"\"\"\n",
    "    Dengue transmission ODE system with vector and human compartments.\n",
    "    \n",
    "    This function implements a SEIR-type model for both vectors and humans:\n",
    "    \n",
    "    Vector compartments:\n",
    "    - Pv: Pre-emergence (pupae/larvae)\n",
    "    - Sv: Susceptible vectors  \n",
    "    - Ev: Exposed vectors (infected but not infectious)\n",
    "    - Qv: Pre-emergence from infected vectors (vertical transmission)\n",
    "    - Iv: Infectious vectors\n",
    "    - Dv: Dead vectors (cumulative)\n",
    "    \n",
    "    Human compartments:\n",
    "    - Sh: Susceptible humans\n",
    "    - Eh: Exposed humans (infected but not infectious) \n",
    "    - Ih: Infectious humans\n",
    "    - Rh: Recovered humans\n",
    "    - Dh: Dead humans (cumulative)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y : array\n",
    "        Current state vector [Pv, Sv, Ev, Qv, Iv, Dv, Sh, Eh, Ih, Rh, Dh]\n",
    "    k_v : float\n",
    "        Vector competence (transmission probability human→vector)\n",
    "    k_h : float  \n",
    "        Human susceptibility (transmission probability vector→human)\n",
    "    pi_v : float\n",
    "        Egg laying rate (temperature-dependent)\n",
    "    theta_v : float\n",
    "        Development rate (temperature-dependent) \n",
    "    b : float\n",
    "        Biting rate (temperature-dependent)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dydt : array\n",
    "        Time derivatives for each compartment\n",
    "    \"\"\"\n",
    "    Pv, Sv, Ev, Qv, Iv, Dv, Sh, Eh, Ih, Rh, Dh = y\n",
    "    \n",
    "    # Calculate population sizes\n",
    "    Nv = Sv + Ev + Iv  # Total vector population\n",
    "    Nh = Sh + Eh + Ih + Rh  # Total human population\n",
    "    \n",
    "    # Density-dependent mortality (competition for resources)\n",
    "    mortality_factor = mu_v * (Nv / (Nh * cc_v))\n",
    "    \n",
    "    # Force of infection (transmission rates)\n",
    "    infection_rate_v = b * k_v * (Ih / Nh)  # Human to vector\n",
    "    infection_rate_h = b * k_h * (Iv / Nh)  # Vector to human\n",
    "    \n",
    "    # Vector compartment dynamics\n",
    "    dpv = egg_lper * pi_v * (Nv - psi_v * Iv) - (mu_e + female_per * theta_v) * Pv\n",
    "    dsv = female_per * theta_v * Pv - (mortality_factor + infection_rate_v) * Sv\n",
    "    dev = infection_rate_v * Sv - (mortality_factor + alpha_v) * Ev\n",
    "    dqv = egg_lper * pi_v * psi_v * Iv - (mu_e + female_per * theta_v) * Qv  # Vertical transmission\n",
    "    div = female_per * theta_v * Qv + alpha_v * Ev - mortality_factor * Iv\n",
    "    ddv = mortality_factor * Iv\n",
    "    \n",
    "    # Human compartment dynamics\n",
    "    dsh = pi_h * Nh - (mu_h + infection_rate_h) * Sh\n",
    "    deh = infection_rate_h * Sh - (mu_h + alpha_h) * Eh\n",
    "    dih = alpha_h * Eh - (mu_h + beta_h + sigma_h) * Ih\n",
    "    drh = beta_h * Ih - mu_h * Rh\n",
    "    ddh = (mu_h + sigma_h) * Ih\n",
    "    \n",
    "    return np.array([dpv, dsv, dev, dqv, div, ddv, dsh, deh, dih, drh, ddh])\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def rk4_step(y, dt, k_v, k_h, pi_v, theta_v, b):\n",
    "    \"\"\"\n",
    "    Fourth-order Runge-Kutta integration step.\n",
    "    \n",
    "    Provides numerical integration of the ODE system with improved\n",
    "    accuracy compared to Euler's method.\n",
    "    \"\"\"\n",
    "    k1 = ode_system(y, k_v, k_h, pi_v, theta_v, b)\n",
    "    k2 = ode_system(y + 0.5 * dt * k1, k_v, k_h, pi_v, theta_v, b)\n",
    "    k3 = ode_system(y + 0.5 * dt * k2, k_v, k_h, pi_v, theta_v, b)\n",
    "    k4 = ode_system(y + dt * k3, k_v, k_h, pi_v, theta_v, b)\n",
    "    \n",
    "    return y + (dt / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4)\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def initial_conditions_fast(tot_cases, tot_pop, sus_per):\n",
    "    \"\"\"\n",
    "    Calculate epidemiologically consistent initial conditions.\n",
    "    \n",
    "    Estimates initial compartment sizes based on observed case data\n",
    "    and epidemiological parameters, assuming quasi-equilibrium.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tot_cases : float\n",
    "        Total observed cases in first week\n",
    "    tot_pop : float\n",
    "        Total population size\n",
    "    sus_per : float\n",
    "        Proportion of susceptible population\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    y0 : array\n",
    "        Initial state vector for ODE integration\n",
    "    \"\"\"\n",
    "    # Estimate initial infectious humans from case data\n",
    "    i0 = max(int((tot_cases / (7 * beta_h)) + 0.5), 1) #Infectious humans\n",
    "    e0 = int((i0 / alpha_h) + 0.5)  # Exposed humans\n",
    "    s0 = int(sus_per * tot_pop  + 0.5)  # Susceptible humans\n",
    "    r0 = int(tot_pop - (i0 + e0 + s0))  # Recovered humans (remainder)\n",
    "\n",
    "    # Calculate vector populations proportional to human infections\n",
    "    ev0 = int(cc_v * e0 + 0.5)  # Exposed vectors\n",
    "    iv0 = int(cc_v * i0 + 0.5)  # Infectious vectors\n",
    "    sv0 = int(cc_v * (s0 + r0) + 0.5)  # Susceptible vectors\n",
    "    qv0 = int(cc_v * (i0 + e0) + 0.5)  # Pre-emergence (vertical transmission)\n",
    "    pv0 = int(cc_v * (s0 + r0) + 0.5)  # Pre-emergence (normal)\n",
    "    \n",
    "    # Final human compartments\n",
    "    sh0 = int(tot_pop - (i0 + e0 + r0))\n",
    "    eh0 = e0\n",
    "    ih0 = i0\n",
    "    rh0 = r0\n",
    "    \n",
    "    return np.array([float(pv0), float(sv0), float(ev0), float(qv0), float(iv0), 0.0, \n",
    "                     float(sh0), float(eh0), float(ih0), float(rh0), 0.0])\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def simulate_dengue_fast(k_v, k_h, sus_per, tot_cases, tot_pop, \n",
    "                        egg_lrate, egg_drate, bite_rate, days):\n",
    "    \"\"\"\n",
    "    Fast simulation of dengue transmission dynamics.\n",
    "    \n",
    "    Integrates the ODE system over the specified time period with\n",
    "    time-varying weather-dependent parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    k_v, k_h, sus_per : float\n",
    "        Model parameters to be estimated\n",
    "    tot_cases, tot_pop : float\n",
    "        Initial epidemic conditions\n",
    "    egg_lrate, egg_drate, bite_rate : array\n",
    "        Time-varying weather-dependent parameters\n",
    "    days : int\n",
    "        Simulation duration\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    results : array\n",
    "        Time series of all compartments [days x 11]\n",
    "    \"\"\"\n",
    "    # Initialize with epidemiologically consistent conditions\n",
    "    y = initial_conditions_fast(tot_cases, tot_pop, sus_per)\n",
    "    \n",
    "    results = np.zeros((days, 11))\n",
    "    results[0] = y\n",
    "    \n",
    "    dt = 1.0  # Daily time step\n",
    "    \n",
    "    # Integrate ODE system day by day\n",
    "    for i in range(1, days):\n",
    "        # Use weather parameters for current day (with bounds checking)\n",
    "        idx = min(i, len(egg_lrate) - 1)\n",
    "        pi_v = egg_lrate[idx]\n",
    "        theta_v = egg_drate[idx]\n",
    "        b = bite_rate[idx]\n",
    "        \n",
    "        # Runge-Kutta integration step\n",
    "        y = rk4_step(y, dt, k_v, k_h, pi_v, theta_v, b)        \n",
    "        # Prevent negative populations\n",
    "        y = np.maximum(y, MIN_VALUE)\n",
    "        \n",
    "        results[i] = y\n",
    "    \n",
    "    return results\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def calculate_weekly_cases_fast(results):\n",
    "    \"\"\"\n",
    "    Calculate weekly case incidence from compartment dynamics.\n",
    "    \n",
    "    Computes new cases by tracking transitions from compartments,\n",
    "    aggregated to weekly totals for comparison with surveillance data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : array\n",
    "        Daily time series from ODE simulation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    weekly_cases : array\n",
    "        Weekly case counts\n",
    "    \"\"\"\n",
    "    Ih = results[:, 8]  # Infectious humans\n",
    "    Rh = results[:, 9]  # Recovered humans  \n",
    "    Dh = results[:, 10] # Dead humans\n",
    "    \n",
    "    days = len(results)\n",
    "    new_cases = np.zeros(days)\n",
    "    \n",
    "    # Calculate daily incidence as net flow out of infectious compartment\n",
    "    new_cases[0] = Ih[0]\n",
    "    for i in range(1, days):\n",
    "        new_Ih = max(0.0, Ih[i] - Ih[i-1])  # New infections\n",
    "        new_Rh = max(0.0, Rh[i] - Rh[i-1])  # New recoveries\n",
    "        new_Dh = max(0.0, Dh[i] - Dh[i-1])  # New deaths\n",
    "        new_cases[i] = new_Ih + new_Rh + new_Dh\n",
    "    \n",
    "    # Aggregate to weekly totals\n",
    "    weeks = days // 7\n",
    "    weekly_cases = np.zeros(weeks)\n",
    "    \n",
    "    for w in range(weeks):\n",
    "        start_idx = w * 7\n",
    "        end_idx = min(start_idx + 7, days)\n",
    "        weekly_cases[w] = np.sum(new_cases[start_idx:end_idx])\n",
    "    \n",
    "    return weekly_cases\n",
    "\n",
    "def simulate_dengue_wrapper(params, egg_lrate, egg_drate, bite_rate, cases_df, days):\n",
    "    \"\"\"\n",
    "    Wrapper function for dengue simulation with data formatting.\n",
    "    \n",
    "    Provides interface between parameter estimation and core simulation,\n",
    "    handling data type conversions and result formatting.\n",
    "    \"\"\"\n",
    "    k_v = params['k_v']\n",
    "    k_h = params['k_h']\n",
    "    sus_per = params['sus_per']\n",
    "    \n",
    "    tot_cases = float(cases_df['casos'].iloc[0])\n",
    "    tot_pop = float(cases_df['pop'].iloc[0])\n",
    "    \n",
    "    # Run core simulation\n",
    "    results = simulate_dengue_fast(k_v, k_h, sus_per, tot_cases, tot_pop,\n",
    "                                  egg_lrate, egg_drate, bite_rate, days)\n",
    "    \n",
    "    # Calculate weekly case incidence\n",
    "    weekly_cases = calculate_weekly_cases_fast(results)\n",
    "    \n",
    "    # Extract population sizes for R0 calculation\n",
    "    Nv = results[-1, 1] + results[-1, 2] + results[-1, 4]  # Vector population\n",
    "    Nh = results[-1, 6] + results[-1, 7] + results[-1, 8] + results[-1, 9]  # Human population\n",
    "    \n",
    "    return weekly_cases, np.full(days, Nv), np.full(days, Nh)\n",
    "\n",
    "@njit(fastmath=True, cache=True)\n",
    "def Basic_Reproduction_Number_fast(k_h, k_v, mean_bite, Nv, Nh):\n",
    "    \"\"\"\n",
    "    Calculate basic reproduction number (R0) for dengue transmission.\n",
    "    \n",
    "    R0 represents the expected number of secondary infections produced by\n",
    "    one infected individual in a completely susceptible population.\n",
    "    \n",
    "    For vector-borne diseases, R0 includes both human-vector and vector-human\n",
    "    transmission cycles.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    k_h, k_v : float\n",
    "        Transmission probabilities\n",
    "    mean_bite : float\n",
    "        Average biting rate\n",
    "    Nv, Nh : float\n",
    "        Vector and human population sizes\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    R0 : float\n",
    "        Basic reproduction number\n",
    "    \"\"\"\n",
    "    if Nh < 1:\n",
    "        Nh = 1\n",
    "    if Nv < 1:\n",
    "        Nv = 1\n",
    "    \n",
    "    # Vector infection probability and duration\n",
    "    term1 = (mean_bite * k_v * Nv) / (Nh * (alpha_v + mu_v * Nv / (Nh * cc_v)))\n",
    "    # Human infection probability\n",
    "    term2 = (mean_bite * k_h) / (alpha_h + mu_h)\n",
    "    # Vector infectious duration\n",
    "    term3 = alpha_v / (mu_v * Nv / (Nh * cc_v))\n",
    "    # Human infectious duration\n",
    "    term4 = alpha_h / (beta_h + mu_h + sigma_h)\n",
    "    \n",
    "    # R0 is geometric mean of transmission cycle components\n",
    "    R0 = np.sqrt(term1 * term2 * term3 * term4)\n",
    "    \n",
    "    return R0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8577782a",
   "metadata": {},
   "source": [
    "Model Persistence and Results Management\n",
    "======================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a574918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_posterior_results(trace, state_cases_df, weather_data_df, city, start_date, end_date, \n",
    "                          best_fit_params, R0_best_fit, save_dir=\"./saved_models/\"):\n",
    "    \"\"\"\n",
    "    Save Bayesian posterior samples and metadata for future forecasting.\n",
    "    \n",
    "    Preserves all information needed to generate forecasts without re-fitting\n",
    "    the model, including parameter samples, training data, and model configuration.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    trace : arviz.InferenceData\n",
    "        PyMC posterior samples from MCMC\n",
    "    state_cases_df : pd.DataFrame\n",
    "        Training case data\n",
    "    weather_data_df : pd.DataFrame\n",
    "        Training weather data\n",
    "    city : str\n",
    "        State/region name\n",
    "    start_date, end_date : str\n",
    "        Training period dates\n",
    "    best_fit_params : dict\n",
    "        Maximum a posteriori parameter estimates\n",
    "    R0_best_fit : float\n",
    "        R0 calculated with best-fit parameters\n",
    "    save_dir : str\n",
    "        Directory for saved model files\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    timestamp : str\n",
    "        Unique identifier for saved model\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract posterior parameter samples for forecasting\n",
    "    posterior_data = {\n",
    "        'k_v_samples': trace.posterior.k_v.values.flatten(),\n",
    "        'k_h_samples': trace.posterior.k_h.values.flatten(),\n",
    "        'sus_per_samples': trace.posterior.sus_per.values.flatten(),\n",
    "        'n_samples': len(trace.posterior.k_v.values.flatten())\n",
    "    }\n",
    "    \n",
    "    # Store model metadata and configuration\n",
    "    metadata = {\n",
    "        'city': city,\n",
    "        'training_start_date': start_date,\n",
    "        'training_end_date': end_date,\n",
    "        'best_fit_params': best_fit_params,\n",
    "        'R0_best_fit': R0_best_fit,\n",
    "        'training_data_shape': state_cases_df.shape,\n",
    "        'weather_coeffs': dict_weather_coeffs,\n",
    "        'model_parameters': {\n",
    "            'egg_lper': egg_lper,\n",
    "            'female_per': female_per,\n",
    "            'mu_v': mu_v,\n",
    "            'psi_v': psi_v,\n",
    "            'mu_e': mu_e,\n",
    "            'alpha_v': alpha_v,\n",
    "            'pi_h': pi_h,\n",
    "            'mu_h': mu_h,\n",
    "            'alpha_h': alpha_h,\n",
    "            'beta_h': beta_h,\n",
    "            'sigma_h': sigma_h,\n",
    "            'cc_v': cc_v\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Store training data for reference\n",
    "    training_data = {\n",
    "        'state_cases_df': state_cases_df.to_dict(),\n",
    "        'weather_data_df': weather_data_df.to_dict()\n",
    "    }\n",
    "    \n",
    "    # Create unique timestamp identifier\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save files with pickle for complex objects, JSON for metadata\n",
    "    with open(f\"{save_dir}posterior_samples_{city}_{timestamp}.pkl\", 'wb') as f:\n",
    "        pickle.dump(posterior_data, f)\n",
    "    \n",
    "    with open(f\"{save_dir}metadata_{city}_{timestamp}.json\", 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "    \n",
    "    with open(f\"{save_dir}training_data_{city}_{timestamp}.pkl\", 'wb') as f:\n",
    "        pickle.dump(training_data, f)\n",
    "    \n",
    "    return timestamp\n",
    "\n",
    "def load_posterior_results(timestamp, city, save_dir=\"./saved_models/\"):\n",
    "    \"\"\"\n",
    "    Load previously saved model results for forecasting.\n",
    "    \n",
    "    Reconstructs all necessary components for generating forecasts from\n",
    "    a previously trained model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    timestamp : str\n",
    "        Model identifier from training\n",
    "    city : str\n",
    "        State/region name\n",
    "    save_dir : str\n",
    "        Directory containing saved models\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    posterior_data : dict\n",
    "        Parameter samples from posterior distribution\n",
    "    metadata : dict\n",
    "        Model configuration and training information\n",
    "    training_data : dict\n",
    "        Original training datasets\n",
    "    \"\"\"\n",
    "    with open(f\"{save_dir}posterior_samples_{city}_{timestamp}.pkl\", 'rb') as f:\n",
    "        posterior_data = pickle.load(f)\n",
    "    \n",
    "    with open(f\"{save_dir}metadata_{city}_{timestamp}.json\", 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    with open(f\"{save_dir}training_data_{city}_{timestamp}.pkl\", 'rb') as f:\n",
    "        training_data = pickle.load(f)\n",
    "    \n",
    "    return posterior_data, metadata, training_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc28de9",
   "metadata": {},
   "source": [
    "Model Training and Parameter Estimation\n",
    "======================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbbb8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_function(state, start_date, end_date, progress_bar_bool=True):\n",
    "    \"\"\"\n",
    "    Fit dengue transmission model using Bayesian parameter estimation.\n",
    "    \n",
    "    This function performs the complete model training pipeline:\n",
    "    1. Data collection and preprocessing\n",
    "    2. Weather parameter calculation\n",
    "    3. Bayesian inference using MCMC\n",
    "    4. Result validation and storage\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    state : str\n",
    "        Brazilian state name for analysis\n",
    "    start_date : str\n",
    "        Training period start date (YYYY-MM-DD)\n",
    "    end_date : str\n",
    "        Training period end date (YYYY-MM-DD)\n",
    "    progress_bar_bool : bool\n",
    "        Show MCMC progress bar\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    timestamp : str\n",
    "        Unique identifier for saved model\n",
    "    best_fit_params : dict\n",
    "        Maximum a posteriori parameter estimates\n",
    "    R0_best_fit : float\n",
    "        Basic reproduction number with best-fit parameters\n",
    "    \"\"\"\n",
    "    # Extend weather data collection to account for initial conditions\n",
    "    weather_start_date = str(datetime.date(datetime.strptime(start_date, \"%Y-%m-%d\") - timedelta(days=2)))\n",
    "    date_difference = datetime.strptime(end_date, \"%Y-%m-%d\") - datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    days = date_difference.days\n",
    "\n",
    "    # Ensure simulation period aligns with weekly surveillance data\n",
    "    if days % 7 != 0:\n",
    "        end_date = str(datetime.date(datetime.strptime(end_date, \"%Y-%m-%d\") - timedelta(days=days % 7) + timedelta(days=7)))\n",
    "        date_difference = datetime.strptime(end_date, \"%Y-%m-%d\") - datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        days = date_difference.days\n",
    "\n",
    "    # Get state-specific municipality codes for data aggregation\n",
    "    geo_data_state = geo_data[geo_data['Nome_UF'] == state]\n",
    "    state_geocodes = geo_data_state['Código Município Completo'].astype(int).tolist()\n",
    "\n",
    "    # Collect epidemiological surveillance data\n",
    "    csv_state_cases_df, major_cities = fetch_dengue_data_state_from_csv(\n",
    "        state_geocodes,\n",
    "        pd.to_datetime(start_date),\n",
    "        pd.to_datetime(end_date)\n",
    "    )\n",
    "\n",
    "    # Collect and process weather data for vector biology parameters\n",
    "    weather_data_df = get_state_weather_data(\n",
    "        weather_start_date,\n",
    "        end_date,\n",
    "        dict_weather_coeffs,\n",
    "        major_cities\n",
    "    )\n",
    "\n",
    "    # Extract time-varying biological parameters for model\n",
    "    bite_rate = np.array(weather_data_df['bite'], dtype=np.float64)\n",
    "    egg_laying_rate = np.array(weather_data_df['egg'], dtype=np.float64)\n",
    "    egg_development_rate = np.array(weather_data_df['theta'], dtype=np.float64)\n",
    "    observed_cases = csv_state_cases_df['casos'].values\n",
    "\n",
    "    # Define PyMC model interface for Bayesian inference\n",
    "    @as_op(itypes=[pt.dscalar, pt.dscalar, pt.dscalar], otypes=[pt.dvector])\n",
    "    def dengue_sim_fast(k_v, k_h, sus_per):\n",
    "        \"\"\"\n",
    "        PyMC interface to dengue simulation model.\n",
    "        \n",
    "        Converts PyMC tensor operations to numpy for efficient computation\n",
    "        while maintaining gradient information for MCMC sampling.\n",
    "        \"\"\"\n",
    "        params = {\"k_v\": float(k_v), \"k_h\": float(k_h), \"sus_per\": float(sus_per)}\n",
    "        try:\n",
    "            weekly_cases, _, _ = simulate_dengue_wrapper(params, egg_laying_rate, egg_development_rate,\n",
    "                                                         bite_rate, csv_state_cases_df, days)\n",
    "            return weekly_cases[:len(csv_state_cases_df)]\n",
    "        except:\n",
    "            # Return small positive values if simulation fails\n",
    "            return np.full(len(csv_state_cases_df), 1e-6)\n",
    "\n",
    "    # Define Bayesian model with informative priors\n",
    "    with pm.Model() as dengue_model:\n",
    "        # Prior distributions based on biological knowledge and literature\n",
    "        k_v = pm.Beta(\"k_v\", alpha=2, beta=20)      # Vector competence (low probability)\n",
    "        k_h = pm.Beta(\"k_h\", alpha=8, beta=5.5)     # Human susceptibility (moderate)\n",
    "        sus_per = pm.Beta(\"sus_per\", alpha=3, beta=20)  # Susceptible proportion (low)\n",
    "\n",
    "        # Link model predictions to observations\n",
    "        cases_mean = dengue_sim_fast(k_v, k_h, sus_per)\n",
    "        cases_safe = pm.math.maximum(cases_mean, 1e-6)  # Prevent zeros\n",
    "        \n",
    "        # Poisson likelihood for case counts (appropriate for count data)\n",
    "        Y_obs = pm.Poisson(\"Y_obs\", mu=cases_safe, observed=observed_cases)\n",
    "\n",
    "        # Use differential evolution MCMC for complex posterior geometry\n",
    "        step = pm.DEMetropolisZ()\n",
    "        trace = pm.sample(draws=5000, tune=5000, chains=6, step=step,\n",
    "                          random_seed=42, discard_tuned_samples=True, \n",
    "                          cores=6, progressbar=progress_bar_bool)\n",
    "\n",
    "    # Extract best-fit parameters (posterior medians for robustness)\n",
    "    k_v_median = np.median(trace.posterior.k_v.values)\n",
    "    k_h_median = np.median(trace.posterior.k_h.values)\n",
    "    sus_per_median = np.median(trace.posterior.sus_per.values)\n",
    "\n",
    "    best_fit_params = {'k_v': k_v_median, 'k_h': k_h_median, 'sus_per': sus_per_median}\n",
    "    \n",
    "    # Generate best-fit trajectory for model validation\n",
    "    weekly_best_fit, Nv_best, Nh_best = simulate_dengue_wrapper(\n",
    "        best_fit_params, egg_laying_rate, egg_development_rate, bite_rate, csv_state_cases_df, days\n",
    "    )\n",
    "\n",
    "    # Calculate epidemiological indicator (R0)\n",
    "    mean_bite = np.mean(bite_rate)\n",
    "    R0_best_fit = Basic_Reproduction_Number_fast(k_h_median, k_v_median, mean_bite, Nv_best[0], Nh_best[0])\n",
    "\n",
    "    # Save complete model results for future use\n",
    "    timestamp = save_posterior_results(\n",
    "        trace, csv_state_cases_df, weather_data_df, state, start_date, end_date,\n",
    "        best_fit_params, R0_best_fit\n",
    "    )\n",
    "    \n",
    "    return timestamp, best_fit_params, R0_best_fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076a8a32",
   "metadata": {},
   "source": [
    "Forecasting and Prediction\n",
    "=========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a616bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forcast_function(state, forecast_start_date, forecast_end_date, metadata, posterior_data):\n",
    "    \"\"\"\n",
    "    Generate probabilistic dengue forecasts using trained model.\n",
    "    \n",
    "    Uses posterior parameter samples to generate ensemble forecasts with\n",
    "    full uncertainty quantification. Handles weather data extrapolation\n",
    "    for forecasts beyond available meteorological data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    state : str\n",
    "        Target state for forecasting\n",
    "    forecast_start_date : str\n",
    "        Forecast period start (YYYY-MM-DD)\n",
    "    forecast_end_date : str\n",
    "        Forecast period end (YYYY-MM-DD)\n",
    "    metadata : dict\n",
    "        Model configuration from training\n",
    "    posterior_data : dict\n",
    "        Parameter samples from Bayesian fitting\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    forecast_df : pd.DataFrame\n",
    "        Probabilistic forecasts with confidence intervals\n",
    "    control_cases : pd.DataFrame\n",
    "        Observed data for forecast validation\n",
    "    \"\"\"\n",
    "    time.sleep(1)  # Rate limiting for API calls\n",
    "\n",
    "    # Parse forecast period\n",
    "    forecast_start = datetime.strptime(forecast_start_date, \"%Y-%m-%d\")\n",
    "    forecast_end = datetime.strptime(forecast_end_date, \"%Y-%m-%d\")\n",
    "    weather_start = forecast_start - timedelta(days=2)\n",
    "\n",
    "    date_difference = forecast_end - forecast_start\n",
    "    forecast_days = date_difference.days\n",
    "\n",
    "    # Align forecast period with weekly reporting\n",
    "    if forecast_days % 7 != 0:\n",
    "        forecast_end = forecast_end - timedelta(days=forecast_days % 7) + timedelta(days=7)\n",
    "        forecast_days = (forecast_end - forecast_start).days\n",
    "    \n",
    "    # Get recent case data for initial conditions\n",
    "    previous_end = forecast_start - timedelta(days=1)\n",
    "    previous_start = previous_end - timedelta(days=30)\n",
    "\n",
    "    # Collect geographic and population data\n",
    "    geo_data_state = geo_data[geo_data['Nome_UF'] == state]\n",
    "    state_geocodes = geo_data_state['Código Município Completo'].astype(int).tolist()\n",
    "    \n",
    "    # Get initial conditions from recent surveillance data\n",
    "    previous_year_cases, _ = fetch_dengue_data_state_from_csv(\n",
    "        state_geocodes,\n",
    "        pd.to_datetime(previous_start),\n",
    "        pd.to_datetime(previous_end)\n",
    "    )\n",
    "    \n",
    "    # Get validation data (observed cases during forecast period)\n",
    "    control_cases, major_cities = fetch_dengue_data_state_from_csv(\n",
    "        state_geocodes,\n",
    "        pd.to_datetime(forecast_start_date),\n",
    "        pd.to_datetime(forecast_end_date)\n",
    "    )\n",
    "    \n",
    "    # Extract initial epidemic conditions\n",
    "    last_week_cases = previous_year_cases.iloc[-1]['casos']\n",
    "    population = previous_year_cases.iloc[-1]['pop']\n",
    "    \n",
    "    # Get weather parameters from training metadata\n",
    "    weather_coeffs = metadata['weather_coeffs']\n",
    "    \n",
    "    # Handle weather data availability (limited by meteorological services)\n",
    "    cutoff_date = datetime.strptime(\"2025-06-09\", \"%Y-%m-%d\")\n",
    "    if forecast_end <= cutoff_date:\n",
    "        # Use real weather data when available\n",
    "        weather_data_forecast = get_state_weather_data(\n",
    "            pd.to_datetime(weather_start.strftime(\"%Y-%m-%d\")), \n",
    "            pd.to_datetime(forecast_end.strftime(\"%Y-%m-%d\")), \n",
    "            weather_coeffs, \n",
    "            major_cities\n",
    "        )\n",
    "    else:\n",
    "        # Extrapolate weather data beyond availability\n",
    "        weather_data_forecast = get_state_weather_data(\n",
    "            pd.to_datetime(weather_start.strftime(\"%Y-%m-%d\")), \n",
    "            pd.to_datetime(cutoff_date.strftime(\"%Y-%m-%d\")), \n",
    "            weather_coeffs, \n",
    "            major_cities\n",
    "        )\n",
    "        \n",
    "        # Prepare for temporal extrapolation\n",
    "        weather_data_forecast = weather_data_forecast.rename_axis('time').reset_index()\n",
    "        weather_data_forecast['time'] = pd.to_datetime(weather_data_forecast['time'])\n",
    "        weather_data_forecast = weather_data_forecast.set_index('time')\n",
    "        full_dates = pd.date_range(start=weather_data_forecast.index.min(), end=forecast_end, freq='D')\n",
    "        weather_data_forecast = weather_data_forecast.reindex(full_dates)\n",
    "        # Linear interpolation for missing weather data\n",
    "        weather_data_forecast = weather_data_forecast.interpolate(method='linear', limit_direction='both')\n",
    "        weather_data_forecast = weather_data_forecast.reset_index().rename(columns={'index': 'date'})\n",
    "\n",
    "    # Extract weather-dependent biological parameters\n",
    "    bite_rate_forecast = np.array(weather_data_forecast['bite'], dtype=np.float64)\n",
    "    egg_laying_rate_forecast = np.array(weather_data_forecast['egg'], dtype=np.float64)\n",
    "    egg_development_rate_forecast = np.array(weather_data_forecast['theta'], dtype=np.float64)\n",
    "    \n",
    "    # Prepare initial conditions for forecast simulation\n",
    "    initial_conditions_df = pd.DataFrame({\n",
    "        'casos': [last_week_cases],\n",
    "        'pop': [population]\n",
    "    })\n",
    "\n",
    "    # Generate ensemble forecasts using posterior samples\n",
    "    n_total_samples = len(posterior_data['k_v_samples'])\n",
    "    forecast_results = []\n",
    "    \n",
    "    # Run forecast simulation for each posterior sample\n",
    "    for i, idx in enumerate(range(n_total_samples)):\n",
    "        sample_params = {\n",
    "            'k_v': posterior_data['k_v_samples'][idx],\n",
    "            'k_h': posterior_data['k_h_samples'][idx],\n",
    "            'sus_per': posterior_data['sus_per_samples'][idx]\n",
    "        } \n",
    "        \n",
    "        # Generate forecast trajectory with current parameter sample\n",
    "        weekly_forecast, Nv_forecast, Nh_forecast = simulate_dengue_wrapper(\n",
    "            sample_params, egg_laying_rate_forecast, egg_development_rate_forecast,\n",
    "            bite_rate_forecast, initial_conditions_df, forecast_days\n",
    "        )\n",
    "        \n",
    "        forecast_results.append(weekly_forecast)\n",
    "\n",
    "    # Convert to array for statistical analysis\n",
    "    forecast_results = np.array(forecast_results)\n",
    "    \n",
    "    # Calculate prediction intervals at multiple confidence levels\n",
    "    confidence_levels = [50, 80, 90, 95]\n",
    "    forecast_stats = {}\n",
    "    for level in confidence_levels:\n",
    "        lower_bound = 50 - level / 2\n",
    "        upper_bound = 50 + level / 2\n",
    "        forecast_stats[f'ci_{level}_lower'] = np.percentile(forecast_results, lower_bound, axis=0)\n",
    "        forecast_stats[f'ci_{level}_upper'] = np.percentile(forecast_results, upper_bound, axis=0)\n",
    "\n",
    "    # Calculate central tendency and dispersion\n",
    "    forecast_stats['median'] = np.median(forecast_results, axis=0)\n",
    "    forecast_stats['std'] = np.std(forecast_results, axis=0)\n",
    "\n",
    "    # Format forecast results\n",
    "    forecast_df = pd.DataFrame({\n",
    "        'week': range(len(forecast_stats['median'])),\n",
    "        'median': forecast_stats['median'],\n",
    "        'std': forecast_stats['std']\n",
    "    })\n",
    "    \n",
    "    # Add confidence intervals\n",
    "    for level in confidence_levels:\n",
    "        forecast_df[f'ci_{level}_lower'] = forecast_stats[f'ci_{level}_lower']\n",
    "        forecast_df[f'ci_{level}_upper'] = forecast_stats[f'ci_{level}_upper']\n",
    "\n",
    "    # Add temporal dimension\n",
    "    forecast_dates = pd.date_range(start=forecast_start, freq='W', periods=len(forecast_stats['median']))\n",
    "    forecast_df['date'] = forecast_dates\n",
    "\n",
    "    return forecast_df, control_cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5846a19",
   "metadata": {},
   "source": [
    "Results Submission and Competition Interface\n",
    "==========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4302f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_forecast(state, forecast_df, adm_1_map,\n",
    "                    model_id=134,\n",
    "                    description_template=\"2024-25 Dengue Forecast for {}\",\n",
    "                    commit_hash='df1c695eaa5af5edb125a9d4ec72a4d2528c7411',\n",
    "                    api_key='DavideNicola:a4ca210d-7d0e-45c9-8cab-1237127d22af'):\n",
    "    \"\"\"\n",
    "    Upload forecast results to digital epidemiology competition platform.\n",
    "    \n",
    "    Formats probabilistic forecasts according to competition requirements\n",
    "    and submits via API for evaluation against held-out surveillance data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    state : str\n",
    "        Brazilian state code\n",
    "    forecast_df : pd.DataFrame\n",
    "        Probabilistic forecast results\n",
    "    adm_1_map : dict\n",
    "        State name to code mapping\n",
    "    model_id : int\n",
    "        Competition model identifier\n",
    "    description_template : str\n",
    "        Forecast description template\n",
    "    commit_hash : str\n",
    "        Model version identifier\n",
    "    api_key : str\n",
    "        Competition platform API key\n",
    "    \"\"\"\n",
    "    predict_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Rename columns to match competition format requirements\n",
    "    df_renamed = forecast_df.rename(columns={\n",
    "        'ci_95_lower': 'lower_95',\n",
    "        'ci_90_lower': 'lower_90',\n",
    "        'ci_80_lower': 'lower_80',\n",
    "        'ci_50_lower': 'lower_50',\n",
    "        'median': 'pred',\n",
    "        'ci_50_upper': 'upper_50',\n",
    "        'ci_80_upper': 'upper_80',\n",
    "        'ci_90_upper': 'upper_90',\n",
    "        'ci_95_upper': 'upper_95'\n",
    "    })\n",
    "\n",
    "    # Ensure all required columns are present\n",
    "    required_columns = ['lower_95', 'lower_90', 'lower_80', 'lower_50',\n",
    "                        'pred', 'upper_50', 'upper_80', 'upper_90', 'upper_95', 'date']\n",
    "\n",
    "    df_final = df_renamed[required_columns].copy()\n",
    "    adm_1_code = adm_1_map[state]\n",
    "\n",
    "    # Submit forecast to competition platform\n",
    "    mosq.upload_prediction(\n",
    "        model_id=model_id,\n",
    "        description=description_template.format(state),\n",
    "        commit=commit_hash,\n",
    "        predict_date=predict_date,\n",
    "        prediction=df_final,\n",
    "        adm_1=adm_1_code,\n",
    "        api_key=api_key\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e859dbf3",
   "metadata": {},
   "source": [
    "Main Execution Pipeline\n",
    "======================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c28ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State administrative mapping for competition submission\n",
    "adm_1_map = {\n",
    "    'Acre': 'AC', 'Alagoas': 'AL', 'Amapá': 'AP', 'Amazonas': 'AM', 'Bahia': 'BA', 'Ceará': 'CE',\n",
    "    'Distrito Federal': 'DF', 'Goiás': 'GO', 'Maranhão': 'MA', 'Mato Grosso': 'MT',\n",
    "    'Mato Grosso do Sul': 'MS', 'Minas Gerais': 'MG', 'Pará': 'PA', 'Paraíba': 'PB',\n",
    "    'Paraná': 'PR', 'Pernambuco': 'PE', 'Piauí': 'PI', 'Rio de Janeiro': 'RJ', 'Rio Grande do Norte': 'RN',\n",
    "    'Rio Grande do Sul': 'RS', 'Rondônia': 'RO', 'Roraima': 'RR', 'Santa Catarina': 'SC',\n",
    "    'São Paulo': 'SP', 'Sergipe': 'SE', 'Tocantins': 'TO'\n",
    "}\n",
    "\n",
    "# Complete list of Brazilian states for analysis ()\n",
    "brazilian_states = [\n",
    "    'Acre', 'Alagoas', 'Amapá', 'Amazonas', 'Bahia', 'Ceará',\n",
    "    'Distrito Federal', 'Goiás', 'Maranhão', 'Mato Grosso',\n",
    "    'Mato Grosso do Sul', 'Minas Gerais', 'Pará', 'Paraíba',\n",
    "    'Paraná', 'Pernambuco', 'Piauí', 'Rio de Janeiro', 'Rio Grande do Norte',\n",
    "    'Rio Grande do Sul', 'Rondônia', 'Roraima', 'Santa Catarina',\n",
    "    'São Paulo', 'Sergipe', 'Tocantins'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fc3d62",
   "metadata": {},
   "source": [
    "Define temporal periods for model training and forecasting evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9192f85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training period: Historical data for parameter estimation\n",
    "train_start_date = \"2020-10-10\"  # Start of training period\n",
    "train_end_date = \"2023-06-30\"    # End of training period\n",
    "\n",
    "# Forecast period: Out-of-sample prediction evaluation\n",
    "forecast_start_date = \"2023-10-08\"  # Start of forecast period\n",
    "forecast_end_date = \"2024-10-06\"    # End of forecast period\n",
    "\n",
    "# Results storage\n",
    "fit_results_by_state = {}      # Training results for each state\n",
    "forecast_results_by_state = {} # Forecast results for each state\n",
    "upload = True                  # Submit to competition platform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad44b79d",
   "metadata": {},
   "source": [
    "Execute complete modeling pipeline for all Brazilian states.\n",
    "\n",
    "This loop performs:\n",
    "1. Bayesian parameter estimation using historical data\n",
    "2. Probabilistic forecasting with uncertainty quantification  \n",
    "3. Competitive submission to digital epidemiology platform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf9d744",
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in brazilian_states:\n",
    "    # === MODEL TRAINING PHASE ===\n",
    "    try:\n",
    "        print(f\"=== Fitting model for {state} ===\")\n",
    "        # Perform Bayesian parameter estimation\n",
    "        timestamp, best_params, R0 = fit_function(state, train_start_date, train_end_date, progress_bar_bool=False)\n",
    "        \n",
    "        # Load fitted model components\n",
    "        posterior_data, metadata, _ = load_posterior_results(timestamp, state)\n",
    "        \n",
    "        # Store training results\n",
    "        fit_results_by_state[state] = {\n",
    "            'timestamp': timestamp,\n",
    "            'best_params': best_params,\n",
    "            'R0': R0,\n",
    "            'posterior_data': posterior_data,\n",
    "            'metadata': metadata\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting state {state}: {e}\")\n",
    "    \n",
    "    # === FORECASTING PHASE ===\n",
    "    try:\n",
    "        print(f\"=== Forecasting model for {state} ===\")\n",
    "        # Extract fitted model components\n",
    "        posterior_data = fit_results_by_state[state]['posterior_data']\n",
    "        metadata = fit_results_by_state[state]['metadata']\n",
    "        \n",
    "        # Generate probabilistic forecasts\n",
    "        forecast_df, control_cases = forcast_function(state, forecast_start_date, forecast_end_date,\n",
    "                                                      metadata, posterior_data)\n",
    "        \n",
    "        # Store forecast results\n",
    "        forecast_results_by_state[state] = {\n",
    "            'forecast': forecast_df,\n",
    "            'observed': control_cases\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error forecasting state {state}: {e}\")    \n",
    "    \n",
    "    # === COMPETITION SUBMISSION ===\n",
    "    if upload is True:\n",
    "        try:\n",
    "            print(f\"=== Uploading results for {state} ===\")\n",
    "            # Submit forecasts to competition platform\n",
    "            upload_forecast(state, forecast_df, adm_1_map)\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading state {state}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
